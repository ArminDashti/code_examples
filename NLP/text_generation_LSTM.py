# This code was generated by chatGPT 3.5

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class LSTMTextGenerator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMTextGenerator, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden):
        output, hidden = self.lstm(x, hidden)
        output = self.fc(output)
        return output, hidden

    def init_hidden(self, batch_size):
        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),
                torch.zeros(self.num_layers, batch_size, self.hidden_size))

input_text = "The quick brown fox jumps over the lazy dog."
chars = sorted(list(set(input_text)))
char_to_index = {ch: i for i, ch in enumerate(chars)}
index_to_char = {i: ch for i, ch in enumerate(chars)}
input_indices = [char_to_index[ch] for ch in input_text]
input_size = len(chars)
hidden_size = 128
output_size = len(chars)
num_layers = 2
seq_length = 25
learning_rate = 0.01
num_epochs = 500
input_tensor = torch.tensor(input_indices).unsqueeze(0)  # Shape: (1, input_length)
target_tensor = torch.tensor(input_indices[1:] + [input_indices[0]])  # Shifted by one
model = LSTMTextGenerator(input_size, hidden_size, output_size, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    hidden = model.init_hidden(1)
    loss = 0
    for i in range(0, input_tensor.size(1) - seq_length, seq_length):
        input_seq = input_tensor[:, i:i+seq_length]
        target_seq = target_tensor[:, i:i+seq_length].reshape(-1)
        output, hidden = model(input_seq.unsqueeze(0), hidden)
        output = output.view(-1, output_size)
        loss += criterion(output, target_seq)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

model.eval()
with torch.no_grad():
    start_letter = 'T'
    generated_text = start_letter
    input_char = start_letter
    hidden = model.init_hidden(1)
    for _ in range(100):
        input_tensor = torch.tensor([[char_to_index[input_char]]]).unsqueeze(0)
        output, hidden = model(input_tensor, hidden)
        output_dist = output.data.view(-1).div(0.8).exp()
        top_i = torch.multinomial(output_dist, 1)[0]
        predicted_char = index_to_char[top_i.item()]
        generated_text += predicted_char
        input_char = predicted_char
        
print("Generated text:")
print(generated_text)
