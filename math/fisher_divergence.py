# Generated by chatGPT

import numpy as np

def fisher_divergence(p, q):
    """
    Compute Fisher divergence between two discrete probability distributions.
    
    Parameters:
        p (numpy.array): Probability distribution 1.
        q (numpy.array): Probability distribution 2.
        
    Returns:
        float: Fisher divergence between p and q.
    """
    # Ensure that p and q are numpy arrays and have the same length
    p = np.array(p)
    q = np.array(q)
    assert len(p) == len(q), "Probability distributions must have the same length"
    
    # Ensure that the distributions sum up to 1
    assert np.isclose(np.sum(p), 1), "Probability distribution p must sum up to 1"
    assert np.isclose(np.sum(q), 1), "Probability distribution q must sum up to 1"
    
    # Compute Fisher divergence
    divergence = np.sum(p * np.log(p / q))
    
    return divergence

# Example usage
p = [0.3, 0.5, 0.2]
q = [0.2, 0.6, 0.2]

fisher_div = fisher_divergence(p, q)
print("Fisher Divergence:", fisher_div)
