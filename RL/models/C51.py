# This code was generated by OpenAI chatGPT 3.5

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class CategoricalDQN(nn.Module):
    def __init__(self, num_actions, num_atoms, V_min, V_max):
        super(CategoricalDQN, self).__init__()
        self.num_actions = num_actions
        self.num_atoms = num_atoms
        self.V_min = V_min
        self.V_max = V_max
        self.support = torch.linspace(V_min, V_max, num_atoms)
        self.delta_z = (V_max - V_min) / (num_atoms - 1)
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_actions * num_atoms)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x.view(-1, self.num_actions, self.num_atoms)

    def get_distribution(self, x):
        return nn.functional.softmax(self.forward(x), dim=-1)

    def q_values(self, x):
        return torch.sum(self.support * self.get_distribution(x), dim=-1)

    def expected_value(self, x):
        return torch.sum(self.support * self.get_distribution(x), dim=-1)

class CategoricalDQNAgent:
    def __init__(self, num_actions, num_atoms, V_min, V_max, gamma=0.99, lr=1e-3):
        self.policy_net = CategoricalDQN(num_actions, num_atoms, V_min, V_max)
        self.target_net = CategoricalDQN(num_actions, num_atoms, V_min, V_max)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.gamma = gamma
        self.num_atoms = num_atoms
        self.V_min = V_min
        self.V_max = V_max

    def train(self, batch):
        states, actions, rewards, next_states, dones = batch
        with torch.no_grad():
            next_dist = self.target_net.get_distribution(next_states)
            next_actions = next_dist.argmax(dim=1, keepdim=True)
            next_dist = next_dist.gather(1, next_actions.repeat(1, 1, self.num_atoms))
            target_distributions = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.gamma * self.support.unsqueeze(0)
            target_distributions = target_distributions.clamp(self.V_min, self.V_max)
            b = (target_distributions - self.V_min) / self.delta_z
            l = b.floor().long()
            u = b.ceil().long()
            offset = torch.linspace(0, (batch_size - 1) * self.num_atoms, batch_size).long().unsqueeze(1).expand(batch_size, self.num_atoms).to(actions)
            proj_dist = torch.zeros(next_dist.size(), device=device)
            proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))
            proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))

        current_dist = self.policy_net.get_distribution(states)
        current_action_dist = current_dist.gather(1, actions.unsqueeze(2).expand(-1, -1, self.num_atoms))
        loss = -torch.sum(proj_dist * torch.log(current_action_dist + 1e-5), dim=-1)
        self.optimizer.zero_grad()
        loss.mean().backward()
        self.optimizer.step()

    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())


num_actions = 4
num_atoms = 51  # Number of atoms in the support distribution
V_min = -10
V_max = 10
input_size = 4  # Example input size
hidden_size = 128
batch_size = 64

agent = CategoricalDQNAgent(num_actions, num_atoms, V_min, V_max)

for epoch in range(num_epochs):
    for batch in data_loader:
        agent.train(batch)
    agent.update_target_network()
