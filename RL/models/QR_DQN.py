# This code was generated by OpenAI chatGPT 3.5


import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


class QR_DQN(nn.Module):
    def __init__(self, input_dim, output_dim, num_quantiles):
        super(QR_DQN, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_quantiles = num_quantiles
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim * num_quantiles)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x.view(-1, self.output_dim, self.num_quantiles)
        
    
class QR_DQNAgent:
    def __init__(self, input_dim, output_dim, num_quantiles, lr=0.001, gamma=0.99, epsilon=0.1):
        self.q_net = QR_DQN(input_dim, output_dim, num_quantiles)
        self.target_net = QR_DQN(input_dim, output_dim, num_quantiles)
        self.target_net.load_state_dict(self.q_net.state_dict())
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.SmoothL1Loss()
        self.gamma = gamma
        self.epsilon = epsilon
        self.num_quantiles = num_quantiles
        
    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.q_net.output_dim)
        else:
            with torch.no_grad():
                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                quantiles = self.q_net(state)
                value = quantiles.mean(dim=2)
                return torch.argmax(value, dim=1).item()
        
    def train(self, batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, batch_weights):
        batch_size = batch_states.shape[0]
        with torch.no_grad():
            next_quantiles = self.target_net(batch_next_states)
            next_value = next_quantiles.mean(dim=2)
            target_value = batch_rewards + self.gamma * (1 - batch_dones) * next_value
        
        self.optimizer.zero_grad()
        quantiles = self.q_net(batch_states)
        action_indices = torch.unsqueeze(batch_actions, dim=1).expand(batch_size, self.num_quantiles)
        chosen_quantiles = torch.gather(quantiles, 1, action_indices.unsqueeze(2)).squeeze(2)
        td_errors = target_value.unsqueeze(2) - chosen_quantiles.unsqueeze(1)
        huber_loss = self.loss_fn(chosen_quantiles, target_value.detach())
        quantile_huber_loss = torch.abs(td_errors) * (torch.abs(td_errors) <= 1).float() + (torch.abs(td_errors) - 0.5) * (torch.abs(td_errors) > 1).float()
        quantile_loss = (batch_weights.unsqueeze(1) * quantile_huber_loss).mean(dim=1).sum(dim=1)
        loss = quantile_loss.mean()
        loss.backward()
        self.optimizer.step()
        return loss.item()
